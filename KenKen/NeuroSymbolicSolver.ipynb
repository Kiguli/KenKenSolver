{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10347,
     "status": "ok",
     "timestamp": 1751922687145,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "vFBTkxd4gGA6",
    "outputId": "8e1ffb6a-fa24-4a53-f1e6-f06bce2c78d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: z3-solver in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.15.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opencv-python) (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: torch==2.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (2.9.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install z3-solver\n",
    "!pip3 install opencv-python\n",
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fg700OCd-OC7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "from z3 import *\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KdoZWLfaECX4"
   },
   "outputs": [],
   "source": [
    "BOARD_SIZE = 900\n",
    "IMG_SIZE = 28\n",
    "SCALE_FACTOR = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4-C2FWh_KhZ"
   },
   "source": [
    "##Loading in character recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MQ0BNl8M-aso"
   },
   "outputs": [],
   "source": [
    "class Grid_CNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Grid_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(262144, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uxv0EWo6mLwC"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(128),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IU2_CC8AfN_A"
   },
   "outputs": [],
   "source": [
    "class CNN_v2(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNN_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(3136, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v73P3k4R-j_e"
   },
   "outputs": [],
   "source": [
    "character_model = CNN_v2(output_dim=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z9v--tCU-wse"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/character_recognition_v2_model_weights.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1751468205975,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "l9v4vSzx-_qS",
    "outputId": "511a2203-3ef1-40a6-f1a3-e34000d5de8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1751468205999,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "9EvIwiNu_ER2",
    "outputId": "e8d2d996-ca81-49cd-a4dd-711fc54e52f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_v2(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_9Gc13E_HC8"
   },
   "outputs": [],
   "source": "# Grid_CNN with 6 output classes: sizes 3, 4, 5, 6, 7, 9\n# Maps: output 0-5 -> sizes 3, 4, 5, 6, 7, 9\ngrid_detection = Grid_CNN(output_dim=6)"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wqbHq5cNmd_0"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/grid_detection_model_weights.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1751468214881,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "zKBYni1omi9O",
    "outputId": "2cf2e73e-7d59-4290-b1bd-953ce231f4bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_detection.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1751468214887,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "0RwaxwHimrfW",
    "outputId": "0415f2ad-da04-47c6-a19b-c78481582da4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grid_CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=262144, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_detection.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sabJ_w8BmlHj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfGHp0YNmvPv"
   },
   "source": [
    "##Get size with Grid CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEf4dSsrmzfl"
   },
   "outputs": [],
   "source": "# Size mapping: output 0-5 -> sizes 3, 4, 5, 6, 7, 9\nLABEL_TO_SIZE = {0: 3, 1: 4, 2: 5, 3: 6, 4: 7, 5: 9}\n\ndef get_size(filename):\n  im = Image.open(filename).convert(\"RGBA\")\n  im = transform(im).unsqueeze(0)\n  output = grid_detection(im)\n  prediction = torch.argmax(output, dim=1).item()\n  return LABEL_TO_SIZE[prediction]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUOzYczSAh0G"
   },
   "source": [
    "##OpenCV grid + border detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2b_u2IWqCxiV"
   },
   "outputs": [],
   "source": [
    "def find_h_borders(h_lines, size, epsilon, delta):\n",
    "  cell_size = ((BOARD_SIZE*SCALE_FACTOR)//size)\n",
    "  vertical_window = (cell_size-delta, cell_size+delta)\n",
    "  h_borders = np.zeros((size-1, size))\n",
    "\n",
    "  horizontal_window = (epsilon, cell_size-epsilon)\n",
    "\n",
    "  for i in range(size-1):\n",
    "    window = h_lines[(h_lines['y1'] >= vertical_window[0]) & (h_lines['y1'] <= vertical_window[1])]\n",
    "    max_val = (window[['y1', 'y2']].min(axis=1)).max()\n",
    "    min_val = (window[['y1', 'y2']].max(axis=1)).min()\n",
    "\n",
    "    if max_val - min_val > int(11 * SCALE_FACTOR):\n",
    "\n",
    "      for j in range(size):\n",
    "\n",
    "        y_vals = window[(np.maximum(window['x1'], window['x2'])>=horizontal_window[0]) & (np.minimum(window['x1'], window['x2'])<=horizontal_window[1])]['y1'].values\n",
    "\n",
    "        if max_val in y_vals or min_val in y_vals:\n",
    "          h_borders[i][j] = 1\n",
    "        horizontal_window = (horizontal_window[0]+cell_size, horizontal_window[1]+cell_size)\n",
    "      horizontal_window = (epsilon, cell_size-epsilon)\n",
    "\n",
    "    vertical_window = (vertical_window[0]+cell_size, vertical_window[1]+cell_size)\n",
    "\n",
    "  return h_borders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7dxqt2EGC4tM"
   },
   "outputs": [],
   "source": [
    "def find_v_borders(v_lines, size, epsilon, delta):\n",
    "  cell_size = ((BOARD_SIZE*SCALE_FACTOR)//size)\n",
    "  horizontal_window = (cell_size-delta, cell_size+delta)\n",
    "  v_borders = np.zeros((size, size-1))\n",
    "\n",
    "  vertical_window = (epsilon, cell_size-epsilon)\n",
    "\n",
    "  for i in range(size-1):\n",
    "    window = v_lines[(v_lines['x1'] >= horizontal_window[0]) & (v_lines['x1'] <= horizontal_window[1])]\n",
    "    max_val = (window[['x1', 'x2']].min(axis=1)).max()\n",
    "    min_val = (window[['x1', 'x2']].max(axis=1)).min()\n",
    "\n",
    "    if max_val - min_val > 11:\n",
    "      for j in range(size):\n",
    "\n",
    "        x_vals = window[(np.maximum(window['y1'], window['y2'])>=vertical_window[0]) & (np.minimum(window['y1'], window['y2'])<=vertical_window[1])]['x1'].values\n",
    "        if max_val in x_vals or min_val in x_vals:\n",
    "          v_borders[j][i] = 1\n",
    "        vertical_window = (vertical_window[0]+cell_size, vertical_window[1]+cell_size)\n",
    "      vertical_window = (epsilon, cell_size-epsilon)\n",
    "    horizontal_window = (horizontal_window[0]+cell_size, horizontal_window[1]+cell_size)\n",
    "\n",
    "  return v_borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mIGW-vpMDdaJ"
   },
   "outputs": [],
   "source": [
    "def make_cage(start_cell, visited, h_borders, v_borders):\n",
    "  cage = [start_cell]\n",
    "  neighbors = [start_cell]\n",
    "  visited[start_cell[0]][start_cell[1]] = 1\n",
    "\n",
    "  while neighbors:\n",
    "    for neighbor in neighbors:\n",
    "      start_cell = neighbor\n",
    "      neighbors = []\n",
    "      if start_cell[1] > 0 and visited[start_cell[0]][start_cell[1]-1] == 0 and v_borders[start_cell[0]][start_cell[1]-1] == 0:\n",
    "        cage.append([start_cell[0], start_cell[1]-1])\n",
    "        visited[start_cell[0]][start_cell[1]-1] = 1\n",
    "        neighbors.append([start_cell[0], start_cell[1]-1])\n",
    "\n",
    "      if start_cell[0] > 0 and visited[start_cell[0]-1][start_cell[1]] == 0 and h_borders[start_cell[0]-1][start_cell[1]] == 0:\n",
    "        cage.append([start_cell[0]-1, start_cell[1]])\n",
    "        visited[start_cell[0]-1][start_cell[1]] = 1\n",
    "        neighbors.append([start_cell[0]-1, start_cell[1]])\n",
    "\n",
    "      if start_cell[1] < len(v_borders)-1 and visited[start_cell[0]][start_cell[1]+1] == 0 and v_borders[start_cell[0]][start_cell[1]] == 0:\n",
    "        cage.append([start_cell[0], start_cell[1]+1])\n",
    "        visited[start_cell[0]][start_cell[1]+1] = 1\n",
    "        neighbors.append([start_cell[0], start_cell[1]+1])\n",
    "\n",
    "      if start_cell[0] < len(v_borders)-1 and visited[start_cell[0]+1][start_cell[1]] == 0 and h_borders[start_cell[0]][start_cell[1]] == 0:\n",
    "        cage.append([start_cell[0]+1, start_cell[1]])\n",
    "        visited[start_cell[0]+1][start_cell[1]] = 1\n",
    "        neighbors.append([start_cell[0]+1, start_cell[1]])\n",
    "  return cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "og25xrHLDUuu"
   },
   "outputs": [],
   "source": [
    "def construct_cages(h_borders, v_borders):\n",
    "  size = len(v_borders)\n",
    "  cages = []\n",
    "  num_cages = 0\n",
    "  visited = np.zeros((size, size))\n",
    "  for row in range(size):\n",
    "    for col in range(size):\n",
    "      start_cell = [row, col]\n",
    "      if visited[row][col] == 0:\n",
    "        cages.append(make_cage(start_cell, visited, h_borders, v_borders))\n",
    "  return cages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9IJI13BBemt9"
   },
   "outputs": [],
   "source": [
    "def get_border_thickness(lines):\n",
    "  v_lines = lines[lines['x1'] == lines['x2']]\n",
    "  return min(v_lines['x1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "M4TiDdm_DfAy"
   },
   "outputs": [],
   "source": [
    "def find_size_and_borders(filename):\n",
    "\n",
    "  #src = cv.imread(cv.samples.findFile(filename), cv.IMREAD_GRAYSCALE)\n",
    "  src = cv.imread(filename)\n",
    "  resized = cv.resize(src, (BOARD_SIZE * SCALE_FACTOR, BOARD_SIZE * SCALE_FACTOR))\n",
    "\n",
    "  filtered = cv.pyrMeanShiftFiltering(resized, sp = 5, sr = 40)\n",
    "  dst = cv.Canny(filtered, 50, 200, None, 3)\n",
    "  cdstP = cv.cvtColor(dst, cv.COLOR_GRAY2BGR)\n",
    "  linesP = cv.HoughLinesP(dst, 1, np.pi / 360, 75, None, 50, 15)\n",
    "  linesP = np.squeeze(linesP, axis=1)\n",
    "  lines_df = pd.DataFrame(linesP, columns=['x1', 'y1', 'x2', 'y2'])\n",
    "  h_lines = lines_df[abs(lines_df['y1'] - lines_df['y2']) < 2]\n",
    "  v_lines = lines_df[abs(lines_df['x1'] - lines_df['x2']) < 2]\n",
    "  border_thickness = get_border_thickness(lines_df)\n",
    "\n",
    "  # tmp = (BOARD_SIZE * SCALE_FACTOR) - max(v_lines[(v_lines['x1']<(BOARD_SIZE * SCALE_FACTOR)-(border_thickness)) & (v_lines[['y1', 'y2']].max(axis=1) > (BOARD_SIZE * SCALE_FACTOR)-(border_thickness*2))]['x1'].values)\n",
    "  # size = round((BOARD_SIZE * SCALE_FACTOR) / tmp)\n",
    "\n",
    "  size = get_size(filename)\n",
    "  cages = construct_cages(find_h_borders(h_lines, size, border_thickness, border_thickness), find_v_borders(v_lines, size, border_thickness, border_thickness))\n",
    "  return size, cages, border_thickness // SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV6KTm8nD0H1"
   },
   "source": [
    "##Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Aa93GCO9D3vG"
   },
   "outputs": [],
   "source": [
    "def get_contours(img):\n",
    "  img = (img * 255).astype(np.uint8)\n",
    "  _, inp = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)\n",
    "\n",
    "  e_kernel = np.ones((1, 1), np.uint8)\n",
    "  inp = cv.erode(inp, e_kernel, iterations=1)\n",
    "  d_kernel = np.ones((3, 3), np.uint8)\n",
    "  inp = cv.dilate(inp, d_kernel, iterations=1)\n",
    "\n",
    "  ctrs, hierarchy = cv.findContours(inp.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "  ctrs = sorted(ctrs, key=lambda cnt: cv.boundingRect(cnt)[0])\n",
    "\n",
    "  #merge overlapping boxes\n",
    "  boxes = [cv.boundingRect(ctrs[0])]\n",
    "  count =1\n",
    "  while count < len(ctrs):\n",
    "      x, y, w, h = boxes[-1]\n",
    "\n",
    "      x2, y2, w2, h2 = cv.boundingRect(ctrs[count])\n",
    "      if x2 < (x+w):\n",
    "        h = abs(y - y2) + h2 if y < y2 else abs(y - y2) + h\n",
    "        w = max(w, w2)\n",
    "        y = min(y, y2)\n",
    "        boxes[-1] = (x, y, w, h)\n",
    "\n",
    "      else:\n",
    "        boxes.append((x2, y2, w2, h2))\n",
    "      count +=1\n",
    "\n",
    "  return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MS3gbkQnD-lY"
   },
   "outputs": [],
   "source": [
    "def get_character(img, box):\n",
    "    char = np.ones((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    x, y, w, h = box\n",
    "    cropped = img[y:y+h, x:x+w]\n",
    "\n",
    "    cropped_img = Image.fromarray((cropped * 255).astype(np.uint8)).convert('L')\n",
    "\n",
    "    aspect = w / h\n",
    "    if aspect > 1:\n",
    "        new_w = IMG_SIZE\n",
    "        new_h = int(IMG_SIZE / aspect)\n",
    "    else:\n",
    "        new_h = IMG_SIZE\n",
    "        new_w = int(IMG_SIZE * aspect)\n",
    "\n",
    "    #print(new_w, new_h)\n",
    "\n",
    "    resized_img = cropped_img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "    canvas = Image.new('L', (IMG_SIZE, IMG_SIZE), color=255)\n",
    "    paste_x = (IMG_SIZE - new_w) // 2\n",
    "    paste_y = (IMG_SIZE - new_h) // 2\n",
    "    canvas.paste(resized_img, (paste_x, paste_y))\n",
    "\n",
    "    result = np.array(canvas).astype(np.float32) / 255.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y239RnLgEQny"
   },
   "outputs": [],
   "source": "def segment_cell(grid, size, border_thickness, row, col):\n  cell_size = len(grid) // size\n\n  # Adaptive height factor: larger boards need more vertical coverage\n  # 3x3-6x6: top 50%, 7x7: top 60%, 9x9+: top 70%\n  if size <= 6:\n    height_factor = 0.5\n  elif size == 7:\n    height_factor = 0.6\n  else:  # 9x9+\n    height_factor = 0.7\n  \n  vertical_end = int(row * cell_size + cell_size * height_factor)\n\n  cell = grid[row*cell_size + border_thickness: vertical_end,\n              col*cell_size + border_thickness: (col+1)*cell_size - border_thickness]\n\n  cell = (cell / 255.0).astype('float64')\n  contours = get_contours(cell)\n  #print(contours)\n  characters = []\n  for box in contours:\n    characters.append(get_character(cell, box))\n\n  return characters"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpGGz_OaEa9p"
   },
   "source": [
    "##Classifying symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rXO92XuLETC_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(characters):\n",
    "  predictions = []\n",
    "  #model.eval()\n",
    "  with torch.no_grad():\n",
    "    for c in characters:\n",
    "      im = torch.tensor(c, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "      output = character_model(im)\n",
    "      predictions.append(torch.argmax(output, dim=1).item())\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3_PXB3REsxp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q19NfxUXE2ee"
   },
   "source": [
    "##Building the symbolic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wxnahn2GFDmj"
   },
   "outputs": [],
   "source": [
    "def make_puzzle(size, border_thickness, cages, filename):\n",
    "  img = Image.open(filename).convert('L')\n",
    "  grid = np.array(img)\n",
    "  puzzle = []\n",
    "  for cage in cages:\n",
    "    puzzle.append({\"cells\":cage, \"op\":\"\", \"target\": 0})\n",
    "    characters = segment_cell(grid, size, border_thickness+5, cage[0][0], cage[0][1])\n",
    "    predictions = get_predictions(characters)\n",
    "    puzzle[-1] = update_puzzle(puzzle[-1], predictions)\n",
    "  return puzzle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2HEkKQdQIlip"
   },
   "outputs": [],
   "source": [
    "def update_puzzle(puzzle, predictions):\n",
    "  if len(predictions) == 1:\n",
    "    puzzle[\"target\"] = predictions[0]\n",
    "  else:\n",
    "    target = 0\n",
    "    for i in range(len(predictions)-1):\n",
    "      power = len(predictions)-2-i\n",
    "      target += predictions[i] * (10 ** power)\n",
    "    if predictions[-1] == 10:\n",
    "      op = \"add\"\n",
    "    elif predictions[-1] == 11:\n",
    "      op = \"div\"\n",
    "    elif predictions[-1] == 12:\n",
    "      op = \"mul\"\n",
    "    elif predictions[-1] == 13:\n",
    "      op = \"sub\"\n",
    "    puzzle[\"target\"] = target\n",
    "    puzzle[\"op\"] = op\n",
    "  return puzzle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLOvVA8kf6pU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH0k_OAhf7HP"
   },
   "source": [
    "##Add Z3 Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01gZ6Y0OgBBp"
   },
   "outputs": [],
   "source": "def parse_block_constraints_optimized(puzzle, cells, size, known_values):\n    \"\"\"\n    Optimized constraint generation with:\n    1. Pre-filled singletons (skip constraint, use known_values)\n    2. Integer-only division (avoid Real arithmetic)\n    3. Domain tightening from cage arithmetic\n    \"\"\"\n    constraints = []\n\n    for block in puzzle:\n        op = block[\"op\"]\n        target = block[\"target\"]\n        block_cells = block[\"cells\"]\n\n        # Get variables, substituting known values\n        vars_in_block = []\n        for i, j in block_cells:\n            if (i, j) in known_values:\n                vars_in_block.append(known_values[(i, j)])\n            else:\n                vars_in_block.append(cells[i][j])\n\n        if op == \"\":\n            # Singleton - already handled via known_values, but add constraint for safety\n            if len(block_cells) == 1:\n                i, j = block_cells[0]\n                if (i, j) not in known_values:\n                    constraints.append(cells[i][j] == target)\n        elif op == \"add\":\n            constraints.append(Sum(vars_in_block) == target)\n            # Domain tightening: each cell <= target - (n-1)\n            n = len(block_cells)\n            if n > 1:\n                max_val = min(size, target - (n - 1))\n                for i, j in block_cells:\n                    if (i, j) not in known_values and max_val < size:\n                        constraints.append(cells[i][j] <= max_val)\n        elif op == \"mul\":\n            if len(vars_in_block) == 1:\n                constraints.append(vars_in_block[0] == target)\n            else:\n                product = vars_in_block[0]\n                for v in vars_in_block[1:]:\n                    product = product * v\n                constraints.append(product == target)\n            # Domain tightening: each cell must divide target\n            for i, j in block_cells:\n                if (i, j) not in known_values:\n                    valid_divisors = [d for d in range(1, size + 1) if target % d == 0]\n                    if len(valid_divisors) < size:\n                        constraints.append(Or([cells[i][j] == d for d in valid_divisors]))\n        elif op == \"sub\" and len(vars_in_block) == 2:\n            a, b = vars_in_block\n            constraints.append(Or(a - b == target, b - a == target))\n        elif op == \"div\" and len(vars_in_block) == 2:\n            a, b = vars_in_block\n            # Integer-only division: avoid Real arithmetic\n            int_target = int(target)\n            constraints.append(Or(a == b * int_target, b == a * int_target))\n        else:\n            raise ValueError(f\"Unsupported operation or malformed block: {block}\")\n\n    return constraints"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOwvY430gBEx"
   },
   "outputs": [],
   "source": "def evaluate_puzzle(puzzle, size):\n    \"\"\"\n    Optimized KenKen solver with:\n    1. Pre-filled singletons\n    2. Integer-only constraints\n    3. Solver tactics for better propagation\n    4. Timeout to avoid infinite solving\n    \"\"\"\n    # Step 1: Extract known values from singletons\n    known_values = {}\n    for block in puzzle:\n        if block[\"op\"] == \"\" and len(block[\"cells\"]) == 1:\n            i, j = block[\"cells\"][0]\n            known_values[(i, j)] = block[\"target\"]\n\n    # Step 2: Create variables only for unknown cells\n    X = [[None for _ in range(size)] for _ in range(size)]\n    for i in range(size):\n        for j in range(size):\n            if (i, j) in known_values:\n                X[i][j] = known_values[(i, j)]  # Use integer directly\n            else:\n                X[i][j] = Int(f\"x_{i+1}_{j+1}\")\n\n    # Step 3: Build constraints\n    constraints = []\n\n    # Cell range constraints (only for unknowns)\n    for i in range(size):\n        for j in range(size):\n            if (i, j) not in known_values:\n                constraints.append(And(1 <= X[i][j], X[i][j] <= size))\n\n    # Row distinctness\n    for i in range(size):\n        constraints.append(Distinct(X[i]))\n\n    # Column distinctness\n    for j in range(size):\n        constraints.append(Distinct([X[i][j] for i in range(size)]))\n\n    # Cage constraints (optimized)\n    constraints.extend(parse_block_constraints_optimized(puzzle, X, size, known_values))\n\n    # Step 4: Create solver with tactics\n    try:\n        tactic = Then('simplify', 'propagate-values', 'solve-eqs', 'smt')\n        s = tactic.solver()\n    except:\n        s = Solver()\n\n    # Set timeout (60 seconds)\n    s.set(\"timeout\", 60000)\n\n    s.add(constraints)\n\n    if s.check() == sat:\n        m = s.model()\n        solution = []\n        for i in range(size):\n            row = []\n            for j in range(size):\n                if (i, j) in known_values:\n                    row.append(known_values[(i, j)])\n                else:\n                    row.append(m.evaluate(X[i][j]))\n            solution.append(row)\n        return solution\n    else:\n        print(\"failed to solve: constraints unsatisfiable\")\n        return None"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcdwSrWHgBQf"
   },
   "source": [
    "##Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WsodEdxPKsLX"
   },
   "outputs": [],
   "source": [
    "filename = \"./board_images/board3_0.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "K_OWbJAwKuVn"
   },
   "outputs": [],
   "source": [
    "size, cages, border_thickness = find_size_and_borders(filename)\n",
    "puzzle = make_puzzle(size, border_thickness, cages, filename)\n",
    "solution = evaluate_puzzle(puzzle, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "f-o0nDrWVT8q"
   },
   "outputs": [],
   "source": [
    "def solve(filename):\n",
    "  size, cages, border_thickness = find_size_and_borders(filename)\n",
    "  puzzle = make_puzzle(size, border_thickness, cages, filename)\n",
    "  solution = evaluate_puzzle(puzzle, size)\n",
    "  return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1751485436670,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "7fWyNzdHgbEU",
    "outputId": "84e80ab0-963b-413e-af1b-a8b768acfc72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2, 1], [1, 3, 2], [2, 1, 3]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1751485438000,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "7u0IGGZe37p4",
    "outputId": "c9528165-40ab-4462-c884-b46d25cd6c0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cells': [[0, 0], [0, 1], [1, 0]], 'op': 'mul', 'target': 6},\n",
       " {'cells': [[0, 2]], 'op': '', 'target': 1},\n",
       " {'cells': [[1, 1], [2, 1]], 'op': 'div', 'target': 3},\n",
       " {'cells': [[1, 2], [2, 2]], 'op': 'sub', 'target': 1},\n",
       " {'cells': [[2, 0]], 'op': '', 'target': 2}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiK6eZIXpHi3"
   },
   "source": [
    "###Avg Time\n",
    "\n",
    "\n",
    "*   3x3: 3s\n",
    "*   4x4: 2s\n",
    "*   5x5: 1s\n",
    "*   6x6: 8s\n",
    "*   7x7: 17s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvu9G-JKj81B"
   },
   "source": [
    "##Testing on full Noto Sans boards dataset\n",
    "\n",
    "Correct performance: solution found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "g7_kd3QAusXb"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Rrvt2U-8kepJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./puzzles/puzzles_dict.json\", \"r\") as f:\n",
    "    puzzles_ds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "IxxR7p_Zm9kA"
   },
   "outputs": [],
   "source": [
    "accuracy = {3:0, 4:0, 5:0, 6:0, 7:0}\n",
    "avg_time = {3:0, 4:0, 5:0, 6:0, 7:0}\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 17904,
     "status": "error",
     "timestamp": 1751391451099,
     "user": {
      "displayName": "Kiersten Brennan",
      "userId": "13284118068611453633"
     },
     "user_tz": 300
    },
    "id": "0l4ssH_7kLd5",
    "outputId": "08acb5c0-ae60-477c-f3fc-a893a4d2d510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0/1\n",
      "3.0/2\n",
      "4.0/3\n",
      "5.0/4\n",
      "6.0/5\n",
      "7.0/6\n",
      "8.0/7\n",
      "9.0/8\n",
      "10.0/9\n",
      "11.0/10\n",
      "12.0/11\n",
      "13.0/12\n",
      "14.0/13\n",
      "15.0/14\n",
      "16.0/15\n",
      "17.0/16\n",
      "18.0/17\n",
      "19.0/18\n",
      "20.0/19\n",
      "21.0/20\n",
      "22.0/21\n",
      "23.0/22\n",
      "24.0/23\n",
      "25.0/24\n",
      "26.0/25\n",
      "27.0/26\n",
      "28.0/27\n",
      "29.0/28\n",
      "30.0/29\n",
      "31.0/30\n",
      "32.0/31\n",
      "33.0/32\n",
      "34.0/33\n",
      "35.0/34\n",
      "36.0/35\n",
      "37.0/36\n",
      "38.0/37\n",
      "39.0/38\n",
      "40.0/39\n",
      "41.0/40\n",
      "42.0/41\n",
      "43.0/42\n",
      "44.0/43\n",
      "45.0/44\n",
      "46.0/45\n",
      "47.0/46\n",
      "48.0/47\n",
      "49.0/48\n",
      "50.0/49\n",
      "51.0/50\n",
      "52.0/51\n",
      "53.0/52\n",
      "54.0/53\n",
      "55.0/54\n",
      "56.0/55\n",
      "57.0/56\n",
      "58.0/57\n",
      "59.0/58\n",
      "60.0/59\n",
      "61.0/60\n",
      "62.0/61\n",
      "63.0/62\n",
      "64.0/63\n",
      "65.0/64\n",
      "66.0/65\n",
      "67.0/66\n",
      "68.0/67\n",
      "69.0/68\n",
      "70.0/69\n",
      "71.0/70\n",
      "72.0/71\n",
      "73.0/72\n",
      "74.0/73\n",
      "75.0/74\n",
      "76.0/75\n",
      "77.0/76\n",
      "78.0/77\n",
      "79.0/78\n",
      "80.0/79\n",
      "81.0/80\n",
      "82.0/81\n",
      "83.0/82\n",
      "84.0/83\n",
      "85.0/84\n",
      "86.0/85\n",
      "87.0/86\n",
      "88.0/87\n",
      "89.0/88\n",
      "90.0/89\n",
      "91.0/90\n",
      "92.0/91\n",
      "93.0/92\n",
      "94.0/93\n",
      "95.0/94\n",
      "96.0/95\n",
      "97.0/96\n",
      "98.0/97\n",
      "99.0/98\n",
      "100.0/99\n",
      "101.0/100\n",
      "2.0/1\n",
      "3.0/2\n",
      "4.0/3\n",
      "5.0/4\n",
      "6.0/5\n",
      "7.0/6\n",
      "8.0/7\n",
      "9.0/8\n",
      "10.0/9\n",
      "11.0/10\n",
      "12.0/11\n",
      "13.0/12\n",
      "14.0/13\n",
      "15.0/14\n",
      "16.0/15\n",
      "17.0/16\n",
      "18.0/17\n",
      "19.0/18\n",
      "20.0/19\n",
      "21.0/20\n",
      "22.0/21\n",
      "23.0/22\n",
      "24.0/23\n",
      "25.0/24\n",
      "26.0/25\n",
      "27.0/26\n",
      "28.0/27\n",
      "29.0/28\n",
      "30.0/29\n",
      "31.0/30\n",
      "32.0/31\n",
      "33.0/32\n",
      "34.0/33\n",
      "35.0/34\n",
      "36.0/35\n",
      "37.0/36\n",
      "38.0/37\n",
      "39.0/38\n",
      "40.0/39\n",
      "41.0/40\n",
      "42.0/41\n",
      "43.0/42\n",
      "44.0/43\n",
      "45.0/44\n",
      "46.0/45\n",
      "47.0/46\n",
      "48.0/47\n",
      "49.0/48\n",
      "50.0/49\n",
      "51.0/50\n",
      "52.0/51\n",
      "53.0/52\n",
      "54.0/53\n",
      "55.0/54\n",
      "56.0/55\n",
      "57.0/56\n",
      "58.0/57\n",
      "59.0/58\n",
      "60.0/59\n",
      "61.0/60\n",
      "62.0/61\n",
      "63.0/62\n",
      "64.0/63\n",
      "65.0/64\n",
      "66.0/65\n",
      "67.0/66\n",
      "68.0/67\n",
      "69.0/68\n",
      "70.0/69\n",
      "71.0/70\n",
      "72.0/71\n",
      "73.0/72\n",
      "74.0/73\n",
      "75.0/74\n",
      "76.0/75\n",
      "77.0/76\n",
      "78.0/77\n",
      "79.0/78\n",
      "80.0/79\n",
      "81.0/80\n",
      "82.0/81\n",
      "83.0/82\n",
      "84.0/83\n",
      "85.0/84\n",
      "86.0/85\n",
      "87.0/86\n",
      "88.0/87\n",
      "89.0/88\n",
      "90.0/89\n",
      "91.0/90\n",
      "92.0/91\n",
      "93.0/92\n",
      "94.0/93\n",
      "95.0/94\n",
      "96.0/95\n",
      "97.0/96\n",
      "98.0/97\n",
      "99.0/98\n",
      "100.0/99\n",
      "101.0/100\n",
      "1/1\n",
      "2/2\n",
      "3/3\n",
      "4/4\n",
      "5/5\n",
      "6/6\n",
      "7/7\n",
      "8/8\n",
      "9/9\n",
      "10/10\n",
      "11/11\n",
      "12/12\n",
      "13/13\n",
      "14/14\n",
      "15/15\n",
      "16/16\n",
      "17/17\n",
      "18/18\n",
      "19/19\n",
      "20/20\n",
      "21/21\n",
      "22/22\n",
      "23/23\n",
      "24/24\n",
      "25/25\n",
      "26/26\n",
      "27/27\n",
      "28/28\n",
      "29/29\n",
      "30/30\n",
      "31/31\n",
      "32/32\n",
      "33/33\n",
      "34/34\n",
      "35/35\n",
      "36/36\n",
      "37/37\n",
      "38/38\n",
      "39/39\n",
      "40/40\n",
      "41/41\n",
      "42/42\n",
      "43/43\n",
      "44/44\n",
      "45/45\n",
      "46/46\n",
      "47/47\n",
      "48/48\n",
      "49/49\n",
      "50/50\n",
      "51/51\n",
      "52/52\n",
      "53/53\n",
      "54/54\n",
      "55/55\n",
      "56/56\n",
      "57/57\n",
      "58/58\n",
      "59/59\n",
      "60/60\n",
      "61/61\n",
      "62/62\n",
      "63/63\n",
      "64/64\n",
      "65/65\n",
      "66/66\n",
      "67/67\n",
      "68/68\n",
      "69/69\n",
      "70/70\n",
      "71/71\n",
      "72/72\n",
      "73/73\n",
      "74/74\n",
      "75/75\n",
      "76/76\n",
      "77/77\n",
      "78/78\n",
      "79/79\n",
      "80/80\n",
      "81/81\n",
      "82/82\n",
      "83/83\n",
      "84/84\n",
      "85/85\n",
      "86/86\n",
      "87/87\n",
      "88/88\n",
      "89/89\n",
      "90/90\n",
      "91/91\n",
      "92/92\n",
      "93/93\n",
      "94/94\n",
      "95/95\n",
      "96/96\n",
      "97/97\n",
      "98/98\n",
      "99/99\n",
      "100/100\n",
      "1/1\n",
      "2/2\n",
      "3/3\n",
      "4/4\n",
      "5/5\n",
      "6/6\n",
      "7/7\n",
      "8/8\n",
      "9/9\n",
      "10/10\n",
      "11/11\n",
      "12/12\n",
      "13/13\n",
      "14/14\n",
      "15/15\n",
      "16/16\n",
      "17/17\n",
      "18/18\n",
      "19/19\n",
      "20/20\n",
      "21/21\n",
      "22/22\n",
      "23/23\n",
      "24/24\n",
      "25/25\n",
      "26/26\n",
      "27/27\n",
      "28/28\n",
      "29/29\n",
      "30/30\n",
      "31/31\n",
      "32/32\n",
      "33/33\n",
      "34/34\n",
      "35/35\n",
      "36/36\n",
      "37/37\n",
      "38/38\n",
      "39/39\n",
      "40/40\n",
      "41/41\n",
      "42/42\n",
      "43/43\n",
      "44/44\n",
      "45/45\n",
      "46/46\n",
      "47/47\n",
      "48/48\n",
      "49/49\n",
      "50/50\n",
      "51/51\n",
      "52/52\n",
      "53/53\n",
      "54/54\n",
      "55/55\n",
      "56/56\n",
      "57/57\n",
      "58/58\n",
      "59/59\n",
      "60/60\n",
      "61/61\n",
      "62/62\n",
      "63/63\n",
      "64/64\n",
      "65/65\n",
      "66/66\n",
      "67/67\n",
      "68/68\n",
      "69/69\n",
      "70/70\n",
      "71/71\n",
      "72/72\n",
      "73/73\n",
      "74/74\n",
      "75/75\n",
      "76/76\n",
      "77/77\n",
      "78/78\n",
      "79/79\n",
      "80/80\n",
      "81/81\n",
      "82/82\n",
      "83/83\n",
      "84/84\n",
      "85/85\n",
      "86/86\n",
      "87/87\n",
      "88/88\n",
      "89/89\n",
      "90/90\n",
      "91/91\n",
      "92/92\n",
      "93/93\n",
      "94/94\n",
      "95/95\n",
      "96/96\n",
      "97/97\n",
      "98/98\n",
      "99/99\n",
      "100/100\n",
      "1/1\n",
      "2/2\n",
      "3/3\n",
      "4/4\n",
      "5/5\n",
      "6/6\n",
      "7/7\n",
      "8/8\n",
      "9/9\n",
      "10/10\n",
      "11/11\n",
      "12/12\n",
      "13/13\n",
      "13/14\n",
      "13/15\n",
      "14/16\n",
      "15/17\n",
      "16/18\n",
      "17/19\n",
      "18/20\n",
      "19/21\n",
      "20/22\n",
      "21/23\n",
      "22/24\n",
      "23/25\n",
      "24/26\n",
      "25/27\n",
      "26/28\n",
      "27/29\n",
      "28/30\n"
     ]
    }
   ],
   "source": [
    "for size in range(3, 8):\n",
    "  for i in range(0, len(puzzles_ds[str(size)])):\n",
    "    filepath= \"./board_images/board\"+str(size)+\"_\"+str(i)+\".png\"\n",
    "    start = time.time()\n",
    "    s, cages, b_t = find_size_and_borders(filepath)\n",
    "    try:\n",
    "      puzzle = make_puzzle(s, b_t, cages, filepath)\n",
    "      solution = evaluate_puzzle(puzzle, s)\n",
    "      if solution:\n",
    "        accuracy[size]+=1\n",
    "    except:\n",
    "      pass\n",
    "    end = time.time()\n",
    "    avg_time[size] += (end - start)\n",
    "    total+=1\n",
    "    print(str(accuracy[size])+\"/\"+str(total))\n",
    "  avg_time[size] = avg_time[size] / total\n",
    "  accuracy[size] = accuracy[size] / total\n",
    "  total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUAYQ4XjsR0u"
   },
   "source": [
    "\n",
    "\n",
    "*   100% 3x3 accuracy\n",
    "*   100% 4x4 accuracy\n",
    "*   100% 5x5 accuracy\n",
    "*   100% 6x6 accuracy\n",
    "*   100% 7x7 accuracy (20 mins for 30 puzzles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RNZsZOKu1L8"
   },
   "source": [
    "##Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHJb0hoOF16r"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'accuracy': accuracy,\n",
    "    'avg_time': avg_time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkAzHwF6u4LY"
   },
   "outputs": [],
   "source": [
    "results.to_csv('./results/neurosymbolic_solver.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPad7jY4w9Tkq84Pk18ENH7",
   "mount_file_id": "1KE7IykSHOI2n4K8EnAuRkTKgo_UMb-se",
   "provenance": [
    {
     "file_id": "14AMs7txD7r33g_XEIlFx7EdKhwQfo1xE",
     "timestamp": 1751486868195
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}